{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fb1839",
   "metadata": {},
   "source": [
    "# Reacher Arm Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c1e55",
   "metadata": {},
   "source": [
    "![Screenshot of reacher arm environment](doc/BannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869a037",
   "metadata": {},
   "source": [
    "This is an implementation of the Deep Deterministic Policy Gradients Algorithm for training a two joint arm to keep its end effector within a moving target volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24340d60",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - N-Step Bootstrapping\n",
    "  - Prioritized Replay\n",
    "  - NoisyNet Layers\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Helpers\n",
    "  - Network Definition\n",
    "  - Training Code\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbce7a5",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bcadd",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file into any convenient directory, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2868f93",
   "metadata": {},
   "source": [
    "### Imports and references\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace, and identify the path to the simulated environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65116bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set to the path to simulated environment executable on system.\n",
    "env_location = \\\n",
    "    \"C:/Projects/UdacityRLp2/Reacher_Windows_x86_64/Reacher_Windows_x86_64/Reacher.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ffd90",
   "metadata": {},
   "source": [
    "## Description of Algorithm\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm extends the application of Q-Learning methods to action spaces with continuously valued dimensions [[1]](#References).  There are two networks involved, a Policy Network and Action Value (Q) network.  During learning, the Policy Network generates an action according to the input state, and both this action and the state are supplied to the Q network as input.  The Q network outputs a single action value, and the gradient of this value with respect to the parameters of the Policy Network are used to nudge the policy towards one with a higher action value (by gradient ascent). <br><br> \n",
    "In typical Deep Q Learning (DQN), each action has a corresponding output from the Q network, but this representation of the Action Value Function $Q(s,a)$ does not naturally accomodate continuously-valued actions.  The primary difference of DDPG with respect to DQN, is that actions are instead explicit, continuously valued inputs to the Action Value function approximation.  This allows closed-form computation of the gradient of $Q(s,a)$ with respect to changes in magnitudes of the continuously-valued action variables.\n",
    "\n",
    "The basic DDPG algorithm reads as follows (from [[1]](#References)): <br>\n",
    "![DDPG Algorithm](doc/DDPG_alg.png) <br>\n",
    "\n",
    "This implementation does not require that the environment steps and learning steps happen at the same time, or in a 1:1 ratio.  It also incorporates the following improvements:\n",
    "\n",
    "### N-Step Bootstrapping\n",
    "\n",
    "The hyperparameter `n_step_order`, determines the value of $n$ in the following alternative Bellman Update target, replacing the definition for $y_i$ in the algorithm above:\n",
    "\n",
    "$$y_i = r_i + \\gamma r_{i+1} + \\gamma^2 r_{i+2} + ... + \\gamma^n r_{i+n} + \\gamma^{n+1} Q'(s_{i+n+1},\\mu '(s_{i+n+1}|\\theta^{\\mu '})|\\theta^{Q'})$$\n",
    "\n",
    "N-Step Bootstrapping increases the relative weight of sampled rewards from the environment, compared to rewards estimated by the Action Value function $Q(s,a)$. Anecdotally, this seems to assist action values propagating backwards in time and through 'bottlenecks' where most nearby states have comparatively low State Values.  So essentially, initial learning can be faster, and some connections may be made that would otherwise take an unacceptably long time to be made without N-Step Bootstrapping.  However, real rewards are stochastic, and an atypically bad or good run of events will  more readily propagate through a Q network with N-Step Bootstrapping.  If an agent quickly changes its behavior between simple, regimented approaches, it is possible the `n_step_order` value in use is too high for the agent's environment.\n",
    "\n",
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameters of the Policy and Q networks.  Like in [[2]](#References), the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\alpha}, \\alpha \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\alpha$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [[2]](#References).\n",
    "<br><br>Qualitatively, the $Loss$ in this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed.\n",
    "\n",
    "### Noise Model\n",
    "For the Policy Net, noisy linear layers will be used instead of additive noise at the output.  Such layers have learnable noise parameters, so the noise level can be adaptively reduced as a deterministic policy develops (See [[3]](#References)).  Because of the inertia in the problem, the input noise to the NoisyNet layers will be generated from an Orstein-Uhlenbeck process (see [[1]](#References))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4222faf",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "#### Environment\n",
    "`state_dim`: Dimension of the observable state space<br>\n",
    "`act_dim`: Dimension of the action space for each agent <br>\n",
    "`num_agnt`: Number of agents in the environment\n",
    "\n",
    "#### Network Models\n",
    "`pol_hid_num`: Number of hidden layers in the Policy Network<br>\n",
    "`pol_hid_size`: Number of neurons in each hidden layer of the Policy Network<br>\n",
    "`noise_init`: Initial noise weights for NoisyNet layers<br>\n",
    "`noise_sigma`: Magnitude of white noise input to Orstein-Uhlenbeck, see [[1]](#References)<br>\n",
    "`noise_theta`: Decay constant for Orstein-Uhlenbeck, see [[1]](#References)<br>\n",
    "`q_hid_num`: Number of hidden layers in the Q Network<br>\n",
    "`q_hid_size`: Number of neurons in each hidden layer of the Q Network<br>\n",
    "\n",
    "#### Reward Parameters\n",
    "`gamma`: Discount factor per step for rewards<br>\n",
    "`n_step_order`: Number of reward steps to directly incorporate into Bellman Update estimate<br>\n",
    "\n",
    "#### Replay Parameters\n",
    "`buf_life`: Buffer will be reset every this many samples<br>\n",
    "`buf_min_size`: Learning will not be allowed unless replay buffer has this many experiences, to avoid overfitting<br>\n",
    "`alpha`: Prioritization strength factor, see [[2]](#References)<br>\n",
    "`beta`: Importance sampling correction coefficient, see [[2]](#References)<br>\n",
    "\n",
    "#### Optimization Parameters\n",
    "`pol_lr`: Learning rate for Policy Network optimizer<br>\n",
    "`q_lr`: Learning rate for Q Network optimizer<br>\n",
    "`lr_int`: Number of environment steps between each learning phase<br>\n",
    "`lr_stps`: How many learning steps are applied during each learning phase<br>\n",
    "`batch_size`: How many experiences are processed by each agent for each learning step<br>\n",
    "`p_tau`: Soft update factor for target Policy Network, applied once every learning step<br>\n",
    "`q_tau`: Soft update factor for the Q Network, applied once every learning step<br>\n",
    "\n",
    "#### Training Parameters\n",
    "`max_eps`: Maximum number of episodes for which to train<br>\n",
    "`avg_wnd_len`: Length (in episodes) of running average buffer for reported performance<br>\n",
    "`rprt_int`: Number of episodes between prints of performance<br>\n",
    "`slv_thresh`: Minimum average score constituting solution of environment, the achievement of which will end the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf8986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Hyperparameters():\n",
    "    def __init__(self,\n",
    "                 state_dim=33,\n",
    "                 act_dim=4,\n",
    "                 num_agnt=20,\n",
    "                 pol_hid_num=2,\n",
    "                 pol_hid_size=150,\n",
    "                 noise_init=0.1,\n",
    "                 noise_sigma=0.2,\n",
    "                 noise_theta=0.01,\n",
    "                 q_hid_num=2,\n",
    "                 q_hid_size=150,\n",
    "                 gamma=0.97,\n",
    "                 n_step_order=1,\n",
    "                 buf_life=1e6,\n",
    "                 buf_min_size=10000,\n",
    "                 alpha=0.6,\n",
    "                 beta=1.0,\n",
    "                 pol_lr=1e-3,\n",
    "                 q_lr=1e-3,\n",
    "                 lr_int=50,\n",
    "                 lr_stps=100,\n",
    "                 batch_size=128,\n",
    "                 p_tau=5e-4,\n",
    "                 q_tau=5e-4,\n",
    "                 max_eps=2000,\n",
    "                 avg_wnd_len=100,\n",
    "                 rprt_int=2,\n",
    "                 slv_thresh=30):\n",
    "        self.state_dim, self.act_dim, self.num_agnt = state_dim, act_dim, num_agnt\n",
    "        self.pol_hid_num, self.pol_hid_size, self.noise_init = pol_hid_num, pol_hid_size, noise_init\n",
    "        self.q_hid_num, self.q_hid_size, self.gamma = q_hid_num, q_hid_size, gamma\n",
    "        self.n_step_order, self.buf_life, self.buf_min_size = n_step_order, buf_life, buf_min_size\n",
    "        self.alpha, self.beta, self.pol_lr = alpha, beta, pol_lr\n",
    "        self.q_lr, self.lr_int, self.lr_stps = q_lr, lr_int, lr_stps\n",
    "        self.batch_size, self.p_tau, self.q_tau = batch_size, p_tau, q_tau\n",
    "        self.max_eps, self.avg_wnd_len, self.rprt_int = max_eps, avg_wnd_len, rprt_int\n",
    "        self.slv_thresh, self.noise_sigma, self.noise_theta = slv_thresh, noise_sigma, noise_theta\n",
    "\n",
    "def_hyp = DDPG_Hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7faf4",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a675fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging function\n",
    "def tensor_check(input,desc,exp_size):\n",
    "    if torch.any(torch.isnan(input)):\n",
    "        print(f'NaNs in {desc}:')\n",
    "        print(input)\n",
    "    if torch.any(torch.isinf(input)):\n",
    "        print(f'Inf in {desc}:')\n",
    "        print(input)\n",
    "    if not (input.size() == torch.Size(exp_size)):\n",
    "        print(f'{desc} has size {tuple(input.size())}, not {exp_size} expected')\n",
    "\n",
    "# Copied from the Lunar Lander dqn_agent.py file of the Udacity repo for course\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "# Object that represents an experience in the experience buffer or a non-leaf node of the sum tree\n",
    "# Experience tuples are stored in the self.data attribute\n",
    "# Based on code in Reference [4]\n",
    "class SumTreeNode():\n",
    "    \n",
    "    def __init__(self,data=None,p_i=0):\n",
    "        self.data = data\n",
    "        self.p_i = p_i\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "    \n",
    "    def update_p(self, delta_p):\n",
    "        self.p_i += delta_p\n",
    "        if self.parent is not None:\n",
    "            self.parent.update_p(delta_p)\n",
    "    \n",
    "    def attach_child(self,child):\n",
    "        if self.data is None:    # Not a leaf node\n",
    "            if self.left_child is None:    # No children, become leaf with cloned data\n",
    "                self.data = child.data\n",
    "                self.update_p(child.p_i - self.p_i)\n",
    "            else:    # Non-leaf node, attach to lower p_i side\n",
    "                if self.left_child.p_i < self.right_child.p_i:\n",
    "                    delegate_node = self.left_child\n",
    "                else:\n",
    "                    delegate_node = self.right_child\n",
    "                delegate_node.attach_child(child)\n",
    "        else:    # self is a leaf-node.  Clone self.data into new child, become non-leaf\n",
    "            self.left_child = SumTreeNode(self.data,self.p_i)\n",
    "            self.data = None\n",
    "            self.right_child = child\n",
    "            self.left_child.parent, self.right_child.parent = self, self     \n",
    "            self.update_p((self.left_child.p_i + self.right_child.p_i)- self.p_i)\n",
    "    \n",
    "    def weighted_retrieve(self,p_samp):\n",
    "        if self.data is not None: # must be a leaf-node\n",
    "            return self\n",
    "        else:\n",
    "            if self.left_child.p_i >= p_samp:\n",
    "                return self.left_child.weighted_retrieve(p_samp)\n",
    "            else:\n",
    "                return self.right_child.weighted_retrieve(p_samp - self.left_child.p_i)\n",
    "        \n",
    "# Experience aggregate\n",
    "Experience = namedtuple('Experience',['state','action','reward','last_state'])\n",
    "        \n",
    "class PrioritizedReplayBuffer():\n",
    "    \n",
    "    def __init__(self,hyp):\n",
    "        self.buf_life = hyp.buf_life\n",
    "        self.alpha = hyp.alpha\n",
    "        self.store = SumTreeNode()\n",
    "        self.sample_count = 0\n",
    "        self.exp_count = 0\n",
    "        self.beta = hyp.beta\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.exp_count\n",
    "    \n",
    "    def add_experience(self, experience, loss):\n",
    "        new_p_i = pow(loss, self.alpha)\n",
    "        self.store.attach_child(SumTreeNode(experience, new_p_i))\n",
    "        self.exp_count += 1\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        sample_keys = (np.random.rand(batch_size)*self.store.p_i).tolist()\n",
    "        samples = ([self.store.weighted_retrieve(p_samp) for p_samp in sample_keys],\n",
    "                   (self.exp_count,self.store.p_i))\n",
    "        self.sample_count += 1\n",
    "        if (self.sample_count >= self.buf_life):\n",
    "            self.sample_count = 0\n",
    "            self.store = SumTreeNode()\n",
    "            self.exp_count = 0\n",
    "            print ('\\nFlushed replay buffer!\\n')\n",
    "        return samples\n",
    "    \n",
    "# Circular buffer for generation of n_step rewards\n",
    "class MultistepBuffer():\n",
    "    def __init__(self,hyp):\n",
    "        self.n_step_order = hyp.n_step_order\n",
    "        self.store = deque(maxlen = hyp.n_step_order + 1)\n",
    "        self.gamma = hyp.gamma\n",
    "    \n",
    "    def add_experience(self,exp):\n",
    "        self.store.append(exp)\n",
    "    \n",
    "    def ready(self):\n",
    "        return len(self.store) == (self.n_step_order + 1)\n",
    "    \n",
    "    def get_n_step_experience(self):\n",
    "        out_state = self.store[0].state\n",
    "        out_action = self.store[0].action\n",
    "        out_reward = \\\n",
    "            sum([((self.store[i].reward) * pow(self.gamma,i)) for i in range(self.n_step_order)])\n",
    "        out_final_state = self.store[-1].state\n",
    "        return SumTreeNode(Experience(out_state, out_action, out_reward, out_final_state),p_i=1)\n",
    "    \n",
    "# Logger for running average\n",
    "class PerformanceLogger():\n",
    "    def __init__(self,avg_wnd_len=100,starting_scores=None):\n",
    "        self.avg_wnd_len = avg_wnd_len\n",
    "        self.scores = starting_scores if starting_scores is not None else []\n",
    "        self.internal_run_avg = 0\n",
    "        \n",
    "    def add_score(self,score):\n",
    "        self.scores.append(score)\n",
    "        self.internal_run_avg += score / self.avg_wnd_len\n",
    "        # Remove tail of running average\n",
    "        if len(self.scores) > self.avg_wnd_len:\n",
    "            self.internal_run_avg -= self.scores[-(self.avg_wnd_len + 1)] / self.avg_wnd_len\n",
    "    \n",
    "    def has_full_window(self):\n",
    "        return len(self.scores) >= self.avg_wnd_len\n",
    "    \n",
    "    def run_avg(self):\n",
    "        return self.internal_run_avg \\\n",
    "                * (1 if self.has_full_window() else (self.avg_wnd_len/len(self.scores)))\n",
    "    \n",
    "# Ohrstein-Uhlenbeck Process Noise Generator\n",
    "class OhrsteinUhlenbeckGen():\n",
    "    def __init__(self,out_dim=1,theta=1.0,sigma=1.0):\n",
    "        self.state = torch.zeros(out_dim,dtype=torch.float32)\n",
    "        self.out_dim = out_dim\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "    def sample(self):\n",
    "        step_noise = self.sigma * \\\n",
    "                         torch.clamp(torch.randn(self.out_dim,dtype=torch.float32),-5.0,5.0)\n",
    "        self.state = ((1.0 - self.theta) * self.state) + step_noise\n",
    "        return step_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62594cf0",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff21130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,noise_params=(0.05,1,1),id_str=''):\n",
    "        super(NoisyLinear,self).__init__()\n",
    "        self.deterministic_linear = nn.Linear(in_dim,out_dim)\n",
    "        self.noisy_weights = \\\n",
    "            nn.Parameter(noise_params[0]*torch.ones(in_dim,out_dim,dtype=torch.float32))\n",
    "        self.noisy_bias = nn.Parameter(noise_params[0]*torch.ones(1,out_dim,dtype=torch.float32))\n",
    "        self.noise_gen_A = OhrsteinUhlenbeckGen(in_dim,noise_params[1],noise_params[2])\n",
    "        self.noise_gen_B = OhrsteinUhlenbeckGen(out_dim,noise_params[1],noise_params[2])\n",
    "        self.counter = 0\n",
    "        self.report_time = 1000\n",
    "        self.id_str = id_str\n",
    "    \n",
    "    def noise_transform(self,x):\n",
    "        # See section 3(b) of reference [3]\n",
    "        tensor_check(x,'Input to noise_transform',tuple(x.size()))\n",
    "        x = torch.mul(torch.sgn(x),torch.sqrt(torch.abs(x)))\n",
    "        tensor_check(x,'Output of noise_transform',tuple(x.size()))\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Generate factorized noise\n",
    "        now_dev = self.noisy_weights.device\n",
    "        in_dim_noise = self.noise_transform(self.noise_gen_A.sample().unsqueeze(dim=1)).to(now_dev)\n",
    "        out_dim_noise = self.noise_transform(self.noise_gen_B.sample().unsqueeze(dim=0)).to(now_dev)\n",
    "        weight_noise = torch.mul(self.noisy_weights,torch.matmul(in_dim_noise,out_dim_noise))\n",
    "        bias_noise = torch.mul(self.noisy_bias,out_dim_noise)\n",
    "    \n",
    "        x = x.float()\n",
    "        det = self.deterministic_linear(x)\n",
    "        noi = torch.matmul(x,weight_noise) + bias_noise\n",
    "        if x.size(dim=0) > 1:\n",
    "            self.counter += 1\n",
    "            if self.counter == self.report_time:\n",
    "                self.counter = 0\n",
    "                print(f'\\n{self.id_str} Deterministic Mag: {torch.sum(torch.abs(det))/det.numel():.4f}')\n",
    "                print(f'{self.id_str} Noise Mag: {torch.sum(torch.abs(noi))/noi.numel():.4f}')\n",
    "        return det + noi\n",
    "\n",
    "# Generic MLP\n",
    "class DDPG_Subnet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_dim, \n",
    "                 out_dim, \n",
    "                 hid_size, \n",
    "                 num_hid,\n",
    "                 squish_output=False,\n",
    "                 noisy=False, \n",
    "                 noise_params=(0.05,1,1)):\n",
    "        super(DDPG_Subnet,self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.BatchNorm1d(in_dim))\n",
    "        layers.append(nn.Linear(in_dim, hid_size))\n",
    "        layers.append(nn.LeakyReLU(negative_slope=0.05))\n",
    "        for i in range(num_hid-1):\n",
    "            if noisy:\n",
    "                layers.append(NoisyLinear(hid_size, hid_size, noise_params,f'Lyr {i+1}'))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hid_size, hid_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.05))\n",
    "        if noisy:\n",
    "            layers.append(NoisyLinear(hid_size, out_dim, noise_params,f'Lyr {num_hid}'))\n",
    "        else:\n",
    "            layers.append(nn.Linear(hid_size, out_dim))\n",
    "        \n",
    "        self.reg_layers = nn.Sequential(*layers)\n",
    "        self.squish_output = squish_output\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.float()\n",
    "        x = self.reg_layers(x)\n",
    "        if self.squish_output:\n",
    "            x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "# Generators\n",
    "def new_pol_net(hyp):\n",
    "    return DDPG_Subnet(hyp.state_dim,\n",
    "                       hyp.act_dim,\n",
    "                       hyp.pol_hid_size,\n",
    "                       hyp.pol_hid_num,\n",
    "                       True,\n",
    "                       False,\n",
    "                       (hyp.noise_init,hyp.noise_theta,hyp.noise_sigma))\n",
    "\n",
    "def new_q_net(hyp):\n",
    "    return DDPG_Subnet(hyp.state_dim + hyp.act_dim,\n",
    "                       1,\n",
    "                       hyp.q_hid_size,\n",
    "                       hyp.q_hid_num,\n",
    "                       False,\n",
    "                       False,\n",
    "                       (0,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabdd11",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3caa249",
   "metadata": {},
   "source": [
    "#### `train_net` Parameters\n",
    "`q_net`: Q network to use for training run<br>\n",
    "`pol_net`: Policy network to use for training run<br>\n",
    "`env`: Environment to use for training run<br>\n",
    "`hyp`: `DDPG_Hyperparameters` object to use for training run<br>\n",
    "\n",
    "#### `train_net` Returns\n",
    "Reference to a `PerformanceLogger` with the score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd32713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to compute importance sampling weight corrections\n",
    "def comp_w_i(experiences, hyp, device):\n",
    "    \n",
    "    p_i_total = experiences[1][1]\n",
    "    buf_N = experiences[1][0]\n",
    "    # Normalized p_i see eqn (1) of reference [3]\n",
    "    p_i = torch.tensor([(e.p_i/p_i_total) for e in experiences[0]]).to(device)\n",
    "    p_i = torch.clamp(p_i,min=1e-9)\n",
    "    w_i = torch.pow(torch.reciprocal(torch.mul(buf_N,p_i)),hyp.beta).unsqueeze(dim=1)\n",
    "    w_i = torch.clamp(w_i,1e-8,1.0)\n",
    "    \n",
    "    return w_i\n",
    "\n",
    "def Q_Loss(q_net, q_trg, pol_trg, experiences, hyp, device):\n",
    "    \n",
    "    init_states = torch.tensor(np.array([e.data.state for e in experiences[0]])).to(device)\n",
    "    actions = torch.vstack([e.data.action for e in experiences[0]]).to(device)\n",
    "    rewards = torch.tensor([e.data.reward for e in experiences[0]]).unsqueeze(dim=1).to(device)\n",
    "    final_states = torch.tensor(np.array([e.data.last_state for e in experiences[0]])).to(device)\n",
    "\n",
    "    q_net_input = torch.cat((init_states,actions),dim=1).float()\n",
    "    #tensor_check(q_net_input,'Q Loss q_net_input',(len(experiences[0]),hyp.state_dim + hyp.act_dim))\n",
    "    q_net_output = q_net(q_net_input)\n",
    "    \n",
    "    q_trg_input = torch.cat((final_states,pol_trg(final_states)),dim=1).float()\n",
    "    #tensor_check(q_trg_input,'q_trg_input',(len(experiences[0]),hyp.state_dim + hyp.act_dim))\n",
    "    q_trg_output = q_trg(q_trg_input)\n",
    "    \n",
    "    disc = pow(hyp.gamma,hyp.n_step_order)\n",
    "    #tensor_check(rewards,'rewards',(len(experiences[0]),1))\n",
    "    q_loss = (rewards + (disc * q_trg_output)) - q_net_output\n",
    "    q_loss = torch.pow(q_loss,2)\n",
    "    #tensor_check(q_loss,'q_loss',(len(experiences[0]),1))\n",
    "    \n",
    "    # Importance sampling weights\n",
    "    w_i = comp_w_i(experiences, hyp, device)\n",
    "    #tensor_check(w_i,'w_i',(len(experiences[0]),1))\n",
    "    \n",
    "    return (torch.mul(q_loss,w_i), w_i)\n",
    "    \n",
    "def Pol_Loss(q_net, pol_net, experiences, hyp, device):\n",
    "    \n",
    "    init_states = torch.tensor(np.array([e.data.state for e in experiences[0]])).to(device)\n",
    "    w_i = comp_w_i(experiences, hyp, device)\n",
    "    q_net_input = torch.cat((init_states,pol_net(init_states)),dim=1).float()\n",
    "    #tensor_check(q_net_input,'Pol Loss q_net_input',(len(experiences[0]),hyp.state_dim+hyp.act_dim))\n",
    "    q_net_output = q_net(q_net_input)\n",
    "    out_loss = torch.mul(-1.0,torch.mean(torch.mul(q_net_output,w_i)))\n",
    "    #tensor_check(out_loss,'out_loss',())\n",
    "    return out_loss\n",
    "    \n",
    "def train_net(pol_net, q_net, env, hyp):\n",
    "    \n",
    "    # Use gpu if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Move primary models to device\n",
    "    pol_net = pol_net.to(device)\n",
    "    q_net = q_net.to(device)\n",
    "    \n",
    "    # Generate target policy and q networks\n",
    "    pol_trg = new_pol_net(hyp).to(device)\n",
    "    q_trg = new_q_net(hyp).to(device)\n",
    "    \n",
    "    # Initialize target networks, copy policy, zeroed q network\n",
    "    soft_update(pol_net, pol_trg, 1.0)\n",
    "    for param in q_trg.parameters():\n",
    "        param.data.fill_(0)\n",
    "        \n",
    "    # Setup optimizers\n",
    "    pol_optim = Adam(pol_net.parameters(), lr=hyp.pol_lr, weight_decay=1e-4)\n",
    "    q_optim = Adam(q_net.parameters(), lr=hyp.q_lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Init counters\n",
    "    lrn_cntr = 0\n",
    "    rpt_cntr = 0\n",
    "    \n",
    "    # Unity ML-Agents Setup\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    replay_buffer=PrioritizedReplayBuffer(hyp)\n",
    "    perf_log = PerformanceLogger(avg_wnd_len = hyp.avg_wnd_len)\n",
    "    \n",
    "    # Put all networks in train mode as there are batch norm layers\n",
    "    pol_net.train()\n",
    "    pol_trg.train()\n",
    "    q_net.train()\n",
    "    q_trg.train()\n",
    "    \n",
    "    # Noise generator\n",
    "    noise_gen = OhrsteinUhlenbeckGen(hyp.act_dim*hyp.num_agnt,hyp.noise_theta,hyp.noise_sigma)\n",
    "    noise_factor = 0.75\n",
    "    noise_decay = 0.92\n",
    "    \n",
    "    for episode in range(hyp.max_eps):\n",
    "        \n",
    "        n_step_bufs = [MultistepBuffer(hyp) for i in range(hyp.num_agnt)]\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = [0]*hyp.num_agnt\n",
    "        dones = [False]*hyp.num_agnt\n",
    "        \n",
    "        while dones.count(True) < hyp.num_agnt:\n",
    "            \n",
    "            pol_net.eval()\n",
    "            actions = pol_net(torch.tensor(states,device=device)).cpu().detach()\n",
    "            act_noise = noise_factor*noise_gen.sample().reshape((hyp.num_agnt, hyp.act_dim))\n",
    "            actions = torch.clamp(torch.add(actions,act_noise),-1.0,1.0)\n",
    "            pol_net.train()\n",
    "            env_info = env.step(actions.numpy())[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            env_dones = env_info.local_done\n",
    "            \n",
    "            for i in range(hyp.num_agnt):\n",
    "                \n",
    "                if dones[i] == False:\n",
    "                    \n",
    "                    scores[i] += rewards[i]\n",
    "                    n_step_bufs[i].add_experience(Experience(states[i],actions[i],rewards[i],None))\n",
    "                    \n",
    "                    if n_step_bufs[i].ready():\n",
    "                        \n",
    "                        n_step_exp = n_step_bufs[i].get_n_step_experience()\n",
    "                        # Compute loss for single state to determine sampling priority\n",
    "                        q_net.eval()\n",
    "                        q_trg.eval()\n",
    "                        pol_trg.eval()\n",
    "                        loss, w_i = Q_Loss(q_net, q_trg, pol_trg, ([n_step_exp],(1,1)), hyp, device)\n",
    "                        pol_trg.train()\n",
    "                        q_trg.train()\n",
    "                        q_net.train()\n",
    "                        n_step_p_i = torch.div(loss,w_i).item()\n",
    "                        replay_buffer.add_experience(n_step_exp.data,n_step_p_i)\n",
    "                    \n",
    "                    dones[i] = env_dones[i]\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            lrn_cntr += 1\n",
    "            if (lrn_cntr % hyp.lr_int) == 0:\n",
    "                \n",
    "                lrn_cntr = 0\n",
    "                if len(replay_buffer) > max(hyp.buf_min_size, hyp.batch_size):\n",
    "                    \n",
    "                    q_perf_log = PerformanceLogger()\n",
    "                    pol_perf_log = PerformanceLogger()\n",
    "                    for l_step in range(hyp.lr_stps):\n",
    "                        \n",
    "                        # Check that a sample op did not trigger reset\n",
    "                        if len(replay_buffer) < max(hyp.buf_min_size, hyp.batch_size):\n",
    "                            break\n",
    "                            \n",
    "                        samp_exp = replay_buffer.sample(hyp.batch_size)\n",
    "                        \n",
    "                        # Q Network Update\n",
    "                        q_optim.zero_grad()\n",
    "                        loss, w_i = Q_Loss(q_net, q_trg, pol_trg, samp_exp, hyp, device)\n",
    "                        mean_loss = torch.mean(loss)\n",
    "                        mean_loss.backward()\n",
    "                        q_optim.step()\n",
    "                        q_perf_log.add_score(mean_loss.item())\n",
    "                        new_p_i = torch.div(loss,w_i).pow(hyp.alpha).squeeze(dim=1)\n",
    "                        \n",
    "                        # Policy Network Update\n",
    "                        q_optim.zero_grad()\n",
    "                        pol_optim.zero_grad()\n",
    "                        loss = Pol_Loss(q_net, pol_net, samp_exp, hyp, device)\n",
    "                        loss.backward()\n",
    "                        pol_optim.step()\n",
    "                        pol_perf_log.add_score(loss.item())\n",
    "                        \n",
    "                        # Update priorities in replay buffer according to losses\n",
    "                        for exp_num in range(hyp.batch_size):\n",
    "                            samp_exp[0][exp_num].update_p( \\\n",
    "                                (new_p_i[exp_num].item() - samp_exp[0][exp_num].p_i))\n",
    "                            \n",
    "                        # Update target networks\n",
    "                        soft_update(q_net, q_trg, hyp.q_tau)\n",
    "                        soft_update(pol_net, pol_trg, hyp.p_tau)\n",
    "                        \n",
    "                        # Output status of optimization periodically\n",
    "                        if ((l_step + 1) % (hyp.lr_stps / 10) == 0):\n",
    "                            q_avg = q_perf_log.run_avg()\n",
    "                            p_avg = pol_perf_log.run_avg()\n",
    "                            print(f'Completed {l_step + 1} of {hyp.lr_stps} learning steps. ' +\n",
    "                                  f'Q Loss = {q_avg:.5f}, Policy Value = {-p_avg:.5f}', end='\\r')\n",
    "        \n",
    "        perf_log.add_score(sum(scores)/len(scores))\n",
    "        noise_factor = noise_factor * noise_decay\n",
    "        \n",
    "        if perf_log.has_full_window() and (perf_log.run_avg() >= hyp.slv_thresh):\n",
    "            print(f'Solved with average score of {perf_log.run_avg()} in {episode+1} episodes')\n",
    "            break\n",
    "        \n",
    "        rpt_cntr += 1\n",
    "        if (rpt_cntr % hyp.rprt_int) == 0:\n",
    "            rpt_cntr = 0\n",
    "            print(f'\\nCompleted {episode + 1} episodes. Average score = {perf_log.run_avg():.3f}')\n",
    "        \n",
    "        if episode == (hyp.max_eps - 1):\n",
    "            print(f'Failed to solve within the maximum of {max_eps} episodes')\n",
    "            break\n",
    "    \n",
    "    return perf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83f325",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 of 100 learning steps. Q Loss = 0.00001, Policy Value = -0.00072\n",
      "Completed 2 episodes. Average score = 0.220\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00001, Policy Value = 0.000835\n",
      "Completed 4 episodes. Average score = 0.194\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00001, Policy Value = 0.00217\n",
      "Completed 6 episodes. Average score = 0.170\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00001, Policy Value = 0.00352\n",
      "Completed 8 episodes. Average score = 0.137\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00001, Policy Value = 0.00471\n",
      "Completed 10 episodes. Average score = 0.117\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00001, Policy Value = 0.00605\n",
      "Completed 12 episodes. Average score = 0.120\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00001, Policy Value = 0.00749\n",
      "Completed 14 episodes. Average score = 0.173\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00001, Policy Value = 0.00908\n",
      "Completed 16 episodes. Average score = 0.211\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00002, Policy Value = 0.01100\n",
      "Completed 18 episodes. Average score = 0.230\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00002, Policy Value = 0.01236\n",
      "Completed 20 episodes. Average score = 0.263\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00002, Policy Value = 0.01446\n",
      "Completed 22 episodes. Average score = 0.293\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00002, Policy Value = 0.01599\n",
      "Completed 24 episodes. Average score = 0.310\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00002, Policy Value = 0.01778\n",
      "Completed 26 episodes. Average score = 0.334\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00002, Policy Value = 0.01901\n",
      "Completed 28 episodes. Average score = 0.350\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00002, Policy Value = 0.02052\n",
      "Completed 30 episodes. Average score = 0.367\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00003, Policy Value = 0.02203\n",
      "Completed 32 episodes. Average score = 0.398\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00003, Policy Value = 0.02372\n",
      "Completed 34 episodes. Average score = 0.413\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00003, Policy Value = 0.02473\n",
      "Completed 36 episodes. Average score = 0.419\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00003, Policy Value = 0.02679\n",
      "Completed 38 episodes. Average score = 0.425\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00003, Policy Value = 0.02823\n",
      "Completed 40 episodes. Average score = 0.424\n",
      "Completed 100 of 100 learning steps. Q Loss = 0.00003, Policy Value = 0.02840\r"
     ]
    }
   ],
   "source": [
    "pol_net = new_pol_net(def_hyp)\n",
    "q_net = new_q_net(def_hyp)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(pol_net, q_net, env, def_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final models\n",
    "torch.save(pol_net,\"pol_net_final.pt\")\n",
    "torch.save(q_net,\"q_net_final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feae780",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d84f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rslt.scores)\n",
    "plt.title('Training Results')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcc45c",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed94fb",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656ce7d",
   "metadata": {},
   "source": [
    "### Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adaa731",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Lillicrap et. al., Continuous control with deep reinforcement learning, [arXiv:1509.02971](https://arxiv.org/abs/1509.02971) <br>\n",
    "[2] Schaul et. al., Prioritized Experience Replay, [arXiv:1511.05952](https://arxiv.org/abs/1511.05952)<br>\n",
    "[3] Fortunato et. al., Noisy Networks for Exploration, [arXiv:1706.10295](https://arxiv.org/abs/1706.10295)<br>\n",
    "[4] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
