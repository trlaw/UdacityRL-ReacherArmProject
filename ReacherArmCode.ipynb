{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fb1839",
   "metadata": {},
   "source": [
    "# Reacher Arm Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c1e55",
   "metadata": {},
   "source": [
    "![Screenshot of reacher arm environment](doc/BannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869a037",
   "metadata": {},
   "source": [
    "This is an implementation of the Deep Deterministic Policy Gradients Algorithm for training a two joint arm to keep its end effector within a moving target volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24340d60",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - Value Distribution\n",
    "  - Parameterized Model\n",
    "  - Prioritized Replay\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Prioritized Replay Buffer\n",
    "  - Action Value Distribution Function (Neural Network)\n",
    "  - Bellman Update (Loss) Computation\n",
    "  - Training Loop\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbce7a5",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bcadd",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file into any convenient directory, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adaa731",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hessel et. al., Rainbow: Combining Improvements in Deep Reinforcement Learning, arXiv:1710.02298 <br>\n",
    "[2] Bellemare et. al., A Distributional Perspective on Reinforcement Learning, arXiv:1707.06887 <br>\n",
    "[3] Schaul et. al., Prioritized Experience Replay, arXiv:1511.05952<br>\n",
    "[4] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n",
    "[5] Fortunato et. al., Noisy Networks for Exploration, arXiv:1706.10295"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
