{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fb1839",
   "metadata": {},
   "source": [
    "# Reacher Arm Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c1e55",
   "metadata": {},
   "source": [
    "![Screenshot of reacher arm environment](doc/BannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869a037",
   "metadata": {},
   "source": [
    "This is an implementation of the Deep Deterministic Policy Gradients Algorithm for training a two joint arm to keep its end effector within a moving target volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24340d60",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - N-Step Bootstrapping\n",
    "  - Prioritized Replay\n",
    "  - NoisyNet Layers\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Helpers\n",
    "  - Network Definition\n",
    "  - Training Code\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbce7a5",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bcadd",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file into any convenient directory, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2868f93",
   "metadata": {},
   "source": [
    "### Imports and references\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace, and identify the path to the simulated environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65116bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "from math import isnan\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set to the path to simulated environment executable on system.\n",
    "env_location = \\\n",
    "\"C:/Projects/UdacityRLp2/Reacher_Windows_x86_64/Reacher_Windows_x86_64/Reacher.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ffd90",
   "metadata": {},
   "source": [
    "## Description of Algorithm\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm extends the application of Q-Learning methods to action spaces with continuously valued dimensions [[1]](#References).  There are two networks involved, a Policy Network and Action Value (Q) network.  During learning, the Policy Network generates an action according to the input state, and both this action and the state are supplied to the Q network as input.  The Q network outputs a single action value, and the gradient of this value with respect to the parameters of the Policy Network are used to nudge the policy towards one with a higher action value (by gradient ascent). <br><br> \n",
    "In typical Deep Q Learning (DQN), each action has a corresponding output from the Q network, but this representation of the Action Value Function $Q(s,a)$ does not naturally accomodate continuously-valued actions.  The primary difference of DDPG with respect to DQN, is that actions are instead explicit, continuously valued inputs to the Action Value function approximation.  This allows closed-form computation of the gradient of $Q(s,a)$ with respect to changes in magnitudes of the continuously-valued action variables.\n",
    "\n",
    "The basic DDPG algorithm reads as follows (from [[1]](#References)): <br>\n",
    "![DDPG Algorithm](doc/DDPG_alg.png) <br>\n",
    "\n",
    "This implementation incorporates the following improvements:\n",
    "\n",
    "### N-Step Bootstrapping\n",
    "\n",
    "The hyperparameter `n_step_order`, determines the value of $n$ in the following alternative Bellman Update target, replacing the definition for $y_i$ in the algorithm above:\n",
    "\n",
    "$$y_i = r_i + \\gamma r_{i+1} + \\gamma^2 r_{i+2} + ... + \\gamma^n r_{i+n} + \\gamma^{n+1} Q'(s_{i+n+1},\\mu '(s_{i+n+1}|\\theta^{\\mu '})|\\theta^{Q'})$$\n",
    "\n",
    "N-Step Bootstrapping increases the relative weight of sampled rewards from the environment, compared to rewards estimated by the Action Value function $Q(s,a)$. Anecdotally, this seems to assist action values propagating backwards in time and through 'bottlenecks' where most nearby states have comparatively low State Values.  So essentially, initial learning can be faster, and some connections may be made that would otherwise take an unacceptably long time to be made without N-Step Bootstrapping.  However, real rewards are stochastic, and an atypically bad or good run of events will  more readily propagate through a Q network with N-Step Bootstrapping.  If an agent quickly changes its behavior between simple, regimented approaches, it is possible the `n_step_order` value in use is too high for the agent's environment.\n",
    "\n",
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameters of the Policy and Q networks.  Like in [[2]](#References), the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\alpha}, \\alpha \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\alpha$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [[2]](#References).\n",
    "<br><br>Qualitatively, the $Loss$ in this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed.\n",
    "\n",
    "### NoisyNet Layers\n",
    "For the Policy Net, noisy linear layers will be used instead of additive noise at the output.  Such layers have learnable noise parameters, so the noise level can be adaptively reduced as a deterministic policy develops (See [[3]](#References))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4222faf",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "#### Environment\n",
    "`state_dim`: Dimension of the observable state space<br>\n",
    "`act_dim`: Dimension of the action space for each agent <br>\n",
    "`num_agnt`: Number of agents in the environment\n",
    "\n",
    "#### Network Models\n",
    "`pol_hid_num`: Number of hidden layers in the Policy Network<br>\n",
    "`pol_hid_size`: Number of neurons in each hidden layer of the Policy Network<br>\n",
    "`noise_init`: Initial magnitude of the noise parameters in the NoisyNet layers<br>\n",
    "`q_hid_num`: Number of hidden layers in the Q Network<br>\n",
    "`q_hid_size`: Number of neurons in each hidden layer of the Q Network<br>\n",
    "\n",
    "#### Reward Parameters\n",
    "`gamma`: Discount factor per step for rewards<br>\n",
    "`n_step_order`: Number of reward steps to directly incorporate into Bellman Update estimate<br>\n",
    "\n",
    "#### Replay Parameters\n",
    "`buf_life`: Buffer will be reset every this many samples<br>\n",
    "`buf_min_size`: Learning will not be allowed unless replay buffer has this many experiences, to avoid overfitting<br>\n",
    "`alpha`: Prioritization strength factor, see [[2]](#References)<br>\n",
    "`beta`: Importance sampling correction coefficient, see [[2]](#References)<br>\n",
    "\n",
    "#### Optimization Parameters\n",
    "`pol_lr`: Learning rate for Policy Network optimizer<br>\n",
    "`q_lr`: Learning rate for Q Network optimizer<br>\n",
    "`lr_int`: Number of environment steps between each learning phase<br>\n",
    "`lr_stps`: How many learning steps are applied during each learning phase<br>\n",
    "`batch_size`: How many experiences are processed by each agent for each learning step<br>\n",
    "`p_tau`: Soft update factor for target Policy Network, applied once every learning step<br>\n",
    "`q_tau`: Soft update factor for the Q Network, applied once every learning step<br>\n",
    "\n",
    "#### Training Parameters\n",
    "`max_eps`: Maximum number of episodes for which to train<br>\n",
    "`avg_wnd_len`: Length (in episodes) of running average buffer for reported performance<br>\n",
    "`rprt_int`: Number of episodes between prints of performance<br>\n",
    "`slv_thresh`: Minimum average score constituting solution of environment, the achievement of which will end the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf8986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Hyperparameters():\n",
    "    def __init__(self,\n",
    "                 state_dim=33,\n",
    "                 act_dim=4,\n",
    "                 num_agnt=20,\n",
    "                 pol_hid_num=2,\n",
    "                 pol_hid_size=300,\n",
    "                 noise_init=0.1,\n",
    "                 q_hid_num=2,\n",
    "                 q_hid_size=300,\n",
    "                 gamma=0.98,\n",
    "                 n_step_order=5,\n",
    "                 buf_life=2500,\n",
    "                 buf_min_size=500,\n",
    "                 alpha=0.6,\n",
    "                 beta=1.0,\n",
    "                 pol_lr=1e-5,\n",
    "                 q_lr=1e-5,\n",
    "                 lr_int=300,\n",
    "                 lr_stps=500,\n",
    "                 batch_size=16,\n",
    "                 p_tau=4e-4,\n",
    "                 q_tau=4e-4,\n",
    "                 max_eps=2000,\n",
    "                 avg_wnd_len=100,\n",
    "                 rprt_int=5,\n",
    "                 slv_thresh=30):\n",
    "        self.state_dim, self.act_dim, self.num_agnt = state_dim, act_dim, num_agnt\n",
    "        self.pol_hid_num, self.pol_hid_size, self.noise_init = pol_hid_num, pol_hid_size, noise_init\n",
    "        self.q_hid_num, self.q_hid_size, self.gamma = q_hid_num, q_hid_size, gamma\n",
    "        self.n_step_order, self.buf_life, self.buf_min_size = n_step_order, buf_life, buf_min_size\n",
    "        self.alpha, self.beta, self.pol_lr = alpha, beta, pol_lr\n",
    "        self.q_lr, self.lr_int, self.lr_stps = q_lr, lr_int, lr_stps\n",
    "        self.batch_size, self.p_tau, self.q_tau = batch_size, p_tau, q_tau\n",
    "        self.max_eps, self.avg_wnd_len, self.rprt_int = max_eps, avg_wnd_len, rprt_int\n",
    "        self.slv_thresh = slv_thresh\n",
    "\n",
    "def_hyp = DDPG_Hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7faf4",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a675fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging function\n",
    "def tensor_check(input,desc,exp_size):\n",
    "    if torch.any(torch.isnan(input)):\n",
    "        print(f'NaNs in {desc}:')\n",
    "        print(input)\n",
    "    if torch.any(torch.isinf(input)):\n",
    "        print(f'Inf in {desc}:')\n",
    "        print(input)\n",
    "    if not (input.size() == torch.size(exp_size)):\n",
    "        print(f'{desc} has size {tuple(input.size())}, not {exp_size} expected')\n",
    "\n",
    "# Copied from the Lunar Lander dqn_agent.py file of the Udacity repo for course\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "# Object that represents an experience in the experience buffer or a non-leaf node of the sum tree\n",
    "# Experience tuples are stored in the self.data attribute\n",
    "# Based on code in Reference [4]\n",
    "class SumTreeNode():\n",
    "    \n",
    "    def __init__(self,data=None,p_i=0):\n",
    "        self.data = data\n",
    "        self.p_i = p_i\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "    \n",
    "    def update_p(self, delta_p):\n",
    "        self.p_i += delta_p\n",
    "        if self.parent is not None:\n",
    "            self.parent.update_p(delta_p)\n",
    "    \n",
    "    def attach_child(self,child):\n",
    "        if self.data is None:    # Not a leaf node\n",
    "            if self.left_child is None:    # No children, become leaf with cloned data\n",
    "                self.data = child.data\n",
    "                self.update_p(child.p_i - self.p_i)\n",
    "            else:    # Non-leaf node, attach to lower p_i side\n",
    "                if self.left_child.p_i < self.right_child.p_i:\n",
    "                    delegate_node = self.left_child\n",
    "                else:\n",
    "                    delegate_node = self.right_child\n",
    "                delegate_node.attach_child(child)\n",
    "        else:    # self is a leaf-node.  Clone self.data into new child, become non-leaf\n",
    "            self.left_child = SumTreeNode(self.data,self.p_i)\n",
    "            self.data = None\n",
    "            self.right_child = child\n",
    "            self.left_child.parent, self.right_child.parent = self, self     \n",
    "            self.update_p((self.left_child.p_i + self.right_child.p_i)- self.p_i)\n",
    "    \n",
    "    def weighted_retrieve(self,p_samp):\n",
    "        if self.data is not None: # must be a leaf-node\n",
    "            return self\n",
    "        else:\n",
    "            if self.left_child.p_i >= p_samp:\n",
    "                return self.left_child.weighted_retrieve(p_samp)\n",
    "            else:\n",
    "                return self.right_child.weighted_retrieve(p_samp - self.left_child.p_i)\n",
    "        \n",
    "# Experience aggregate\n",
    "Experience = namedtuple('Experience',['state','action','reward','last_state'])\n",
    "        \n",
    "class PrioritizedReplayBuffer():\n",
    "    \n",
    "    def __init__(self,hyp):\n",
    "        self.buf_life = hyp.buf_life\n",
    "        self.alpha = hyp.alpha\n",
    "        self.store = SumTreeNode()\n",
    "        self.sample_count = 0\n",
    "        self.exp_count = 0\n",
    "        self.beta = hyp.beta\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.exp_count\n",
    "    \n",
    "    def add_experience(self, experience, loss):\n",
    "        new_p_i = pow(loss, self.alpha)\n",
    "        self.store.attach_child(SumTreeNode(experience, new_p_i))\n",
    "        self.exp_count += 1\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        sample_keys = (np.random.rand(batch_size)*self.store.p_i).tolist()\n",
    "        samples = ([self.store.weighted_retrieve(p_samp) for p_samp in sample_keys],\n",
    "                   (self.exp_count,self.beta,self.store.p_i))\n",
    "        self.sample_count += 1\n",
    "        if (self.sample_count >= self.buffer_life):\n",
    "            self.sample_count = 0\n",
    "            self.store = SumTreeNode()\n",
    "            self.exp_count = 0\n",
    "            print ('Flushed replay buffer!')\n",
    "        return samples\n",
    "    \n",
    "# Circular buffer for generation of n_step rewards\n",
    "class MultistepBuffer():\n",
    "    def __init__(self,hyp):\n",
    "        self.n_step_order = hyp.n_step_order\n",
    "        self.store = deque(maxlen = hyp.n_step_order + 1)\n",
    "        self.gamma = hyp.gamma\n",
    "    \n",
    "    def add_experience(self,exp):\n",
    "        self.store.append(exp)\n",
    "    \n",
    "    def ready(self):\n",
    "        return len(self.store) == (self.n_step_order + 1)\n",
    "    \n",
    "    def get_n_step_experience(self):\n",
    "        out_state = self.store[0].state\n",
    "        out_action = self.store[0].action\n",
    "        out_reward = \\\n",
    "            sum([((self.store[i].reward) * pow(self.gamma,i)) for i in range(self.n_step_order)])\n",
    "        out_final_state = self.store[-1].state\n",
    "        return SumTreeNode(Experience(out_state, out_action, out_reward, out_final_state),p_i=1)\n",
    "    \n",
    "# Logger for running average\n",
    "class PerformanceLogger():\n",
    "    def __init__(self,avg_wnd_len=100,starting_scores=None):\n",
    "        self.avg_wnd_len = avg_wnd_len\n",
    "        self.scores = starting_scores if starting_scores is not None else []\n",
    "        self.internal_run_avg = 0\n",
    "        \n",
    "    def add_score(self,score):\n",
    "        self.scores.append(score)\n",
    "        self.internal_run_avg += score / self.avg_wnd_len\n",
    "        # Remove tail of running average\n",
    "        if len(self.scores) > self.avg_wnd_len:\n",
    "            self.internal_run_avg -= self.scores[-(self.avg_wnd_len + 1)] / self.avg_wnd_len\n",
    "    \n",
    "    def has_full_window(self):\n",
    "        return len(self.scores) >= self.avg_wnd_len\n",
    "    \n",
    "    def run_avg(self):\n",
    "        return self.internal_run_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62594cf0",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff21130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,noise_init=0.05):\n",
    "        super(NoisyLinear,self).__init__()\n",
    "        self.deterministic_linear = nn.Linear(in_dim,out_dim)\n",
    "        self.noisy_weights = \\\n",
    "            nn.Parameter(noise_init*torch.ones(in_dim,out_dim,dtype=torch.float32))\n",
    "        self.noisy_bias = nn.Parameter(noise_init*torch.ones(1,out_dim,dtype=torch.float32))\n",
    "    \n",
    "    def noise_transform(self,x):\n",
    "        # See section 3(b) of reference [3]\n",
    "        tensor_check(x,'Input to noise_transform',tuple(x.size()))\n",
    "        x = torch.mul(torch.sgn(x),torch.sqrt(torch.abs(x)))\n",
    "        tensor_check(x,'Output of noise_transform',tuple(x.size()))\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Generate factorized gaussian noise, clamping to +/- 5 sigma\n",
    "        in_dim_noise = \\\n",
    "            self.noise_transform( \\\n",
    "                torch.clamp( \\\n",
    "                    torch.randn((self.noisy_weights.size(dim=0),1),device=x.device),-5.0,5.0))\n",
    "        out_dim_noise = \\\n",
    "            self.noise_transform( \\\n",
    "                torch.clamp( \\\n",
    "                    torch.randn((1,self.noisy_weights.size(dim=1)),device=x.device),-5.0,5.0))\n",
    "        \n",
    "        weight_noise = torch.mul(self.noisy_weights,torch.matmul(in_dim_noise,out_dim_noise))\n",
    "        bias_noise = torch.mul(self.noisy_bias,out_dim_noise)\n",
    "    \n",
    "        x = x.float()\n",
    "        return self.deterministic_linear(x) + torch.matmul(x,weight_noise) + bias_noise\n",
    "\n",
    "# Generic MLP\n",
    "class DDPG_Subnet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_dim, \n",
    "                 out_dim, \n",
    "                 hid_size, \n",
    "                 num_hid,\n",
    "                 squish_output=False,\n",
    "                 noisy=False, \n",
    "                 noise_init=0.05):\n",
    "        super(DDPG_Subnet,self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        if noisy:\n",
    "            layers.append(NoisyLinear(in_dim, hid_size, noise_init))\n",
    "        else:\n",
    "            layers.append(nn.Linear(in_dim, hid_size))\n",
    "        layers.append(nn.LeakyReLU(negative_slope=0.05))\n",
    "        for i in range(num_hid-1):\n",
    "            if noisy:\n",
    "                layers.append(NoisyLinear(hid_size, hid_size, noise_init))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hid_size, hid_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.05))\n",
    "        if noisy:\n",
    "            layers.append(NoisyLinear(hid_size, out_dim, noise_init))\n",
    "        else:\n",
    "            layers.append(nn.Linear(hid_size, out_dim))\n",
    "        \n",
    "        self.reg_layers = nn.Sequential(*layers)\n",
    "        self.squish_output = squish_output\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.reg_layers(x)\n",
    "        if self.squish_output:\n",
    "            x = tanh(x)\n",
    "        return x\n",
    "    \n",
    "# Instantiate\n",
    "pol_net = DDPG_Subnet(def_hyp.state_dim,\n",
    "                      def_hyp.act_dim,\n",
    "                      def_hyp.pol_hid_size,\n",
    "                      def_hyp.pol_hid_num,\n",
    "                      True,\n",
    "                      True,\n",
    "                      def_hyp.noise_init)\n",
    "\n",
    "q_net = DDPG_Subnet(def_hyp.state_dim + def_hyp.act_dim,\n",
    "                    1,\n",
    "                    def_hyp.q_hid_size,\n",
    "                    def_hyp.q_hid_num,\n",
    "                    False,\n",
    "                    False,\n",
    "                    0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabdd11",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7947b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1adaa731",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Lillicrap et. al., Continuous control with deep reinforcement learning, [arXiv:1509.02971](https://arxiv.org/abs/1509.02971) <br>\n",
    "[2] Schaul et. al., Prioritized Experience Replay, [arXiv:1511.05952](https://arxiv.org/abs/1511.05952)<br>\n",
    "[3] Fortunato et. al., Noisy Networks for Exploration, [arXiv:1706.10295](https://arxiv.org/abs/1706.10295)<br>\n",
    "[4] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
