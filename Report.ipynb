{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fb1839",
   "metadata": {},
   "source": [
    "# Reacher Arm Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c1e55",
   "metadata": {},
   "source": [
    "![Screenshot of reacher arm environment](doc/BannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869a037",
   "metadata": {},
   "source": [
    "This is an implementation of the Deep Deterministic Policy Gradients Algorithm for training a two joint arm to keep its end effector within a moving target volume.  The agent actions are four continuous inputs corresponding to the torques on two joints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24340d60",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - N-Step Bootstrapping\n",
    "  - Prioritized Replay\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Helpers\n",
    "  - Network Definition\n",
    "  - Training Code\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbce7a5",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bcadd",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file into any convenient directory, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2868f93",
   "metadata": {},
   "source": [
    "### Imports and references\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace, and identify the path to the simulated environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65116bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set to the path to simulated environment executable on system.\n",
    "env_location = \\\n",
    "    \"C:/Projects/UdacityRLp2/Reacher_Windows_x86_64/Reacher_Windows_x86_64/Reacher.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ffd90",
   "metadata": {},
   "source": [
    "## Description of Algorithm\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm extends the application of Q-Learning methods to action spaces with continuously valued dimensions [[1]](#References).  There are two networks involved, a Policy Network and Action Value (Q) network.  During learning, the Policy Network generates an action according to the input state, and both this action and the state are supplied to the Q network as input.  The Q network outputs a single action value, and the gradient of this value with respect to the parameters of the Policy Network are used to nudge the policy towards one with a higher action value (by gradient ascent). <br><br> \n",
    "In typical Deep Q Learning (DQN), each action has a corresponding output from the Q network, but this representation of the Action Value Function $Q(s,a)$ does not naturally accomodate continuously-valued actions.  The primary difference of DDPG with respect to DQN, is that actions are instead explicit, continuously valued inputs to the Action Value function approximation.  This allows closed-form computation of the gradient of $Q(s,a)$ with respect to changes in magnitudes of the continuously-valued action variables.\n",
    "\n",
    "The basic DDPG algorithm reads as follows (from [[1]](#References)): <br>\n",
    "![DDPG Algorithm](doc/DDPG_alg.png) <br>\n",
    "\n",
    "This implementation does not require that the environment steps and learning steps happen at the same time, or in a 1:1 ratio.  It also incorporates the following improvements:\n",
    "\n",
    "### N-Step Bootstrapping\n",
    "\n",
    "The hyperparameter `n_step_order`, determines the value of $n$ in the following alternative Bellman Update target, replacing the definition for $y_i$ in the algorithm above:\n",
    "\n",
    "$$y_i = r_i + \\gamma r_{i+1} + \\gamma^2 r_{i+2} + ... + \\gamma^n r_{i+n} + \\gamma^{n+1} Q'(s_{i+n+1},\\mu '(s_{i+n+1}|\\theta^{\\mu '})|\\theta^{Q'})$$\n",
    "\n",
    "N-Step Bootstrapping increases the relative weight of sampled rewards from the environment, compared to rewards estimated by the Action Value function $Q(s,a)$. Anecdotally, this seems to assist action values propagating backwards in time and through 'bottlenecks' where most nearby states have comparatively low State Values.  So essentially, initial learning can be faster, and some connections may be made that would otherwise take an unacceptably long time to be made without N-Step Bootstrapping.  However, real rewards are stochastic, and an atypically bad or good run of events will  more readily propagate through a Q network with N-Step Bootstrapping.  If an agent quickly changes its behavior between simple, regimented approaches, it is possible the `n_step_order` value in use is too high for the agent's environment.\n",
    "\n",
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameters of the Policy and Q networks.  Like in [[2]](#References), the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\alpha}, \\alpha \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\alpha$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [[2]](#References).\n",
    "<br><br>Qualitatively, the $Loss$ in this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4222faf",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "#### Environment\n",
    "`state_dim`: Dimension of the observable state space<br>\n",
    "`act_dim`: Dimension of the action space for each agent <br>\n",
    "`num_agnt`: Number of agents in the environment\n",
    "\n",
    "#### Network Models\n",
    "`pol_hid_num`: Number of hidden layers in the Policy Network<br>\n",
    "`pol_hid_size`: Number of neurons in each hidden layer of the Policy Network<br>\n",
    "`noise_init`: Initial noise weights for NoisyNet layers<br>\n",
    "`noise_sigma`: Magnitude of white noise input to Orstein-Uhlenbeck, see [[1]](#References)<br>\n",
    "`noise_theta`: Decay constant for Orstein-Uhlenbeck, see [[1]](#References)<br>\n",
    "`q_hid_num`: Number of hidden layers in the Q Network<br>\n",
    "`q_hid_size`: Number of neurons in each hidden layer of the Q Network<br>\n",
    "\n",
    "#### Reward Parameters\n",
    "`gamma`: Discount factor per step for rewards<br>\n",
    "`n_step_order`: Number of reward steps to directly incorporate into Bellman Update estimate<br>\n",
    "\n",
    "#### Replay Parameters\n",
    "`buf_life`: Buffer will be reset every this many samples<br>\n",
    "`buf_min_size`: Learning will not be allowed unless replay buffer has this many experiences, to avoid overfitting<br>\n",
    "`alpha`: Prioritization strength factor, see [[2]](#References)<br>\n",
    "`beta`: Importance sampling correction coefficient, see [[2]](#References)<br>\n",
    "\n",
    "#### Optimization Parameters\n",
    "`pol_lr`: Learning rate for Policy Network optimizer<br>\n",
    "`q_lr`: Learning rate for Q Network optimizer<br>\n",
    "`lr_int`: Number of environment steps between each learning phase<br>\n",
    "`lr_stps`: How many learning steps are applied during each learning phase<br>\n",
    "`batch_size`: How many experiences are processed by each agent for each learning step<br>\n",
    "`p_tau`: Soft update factor for target Policy Network, applied once every learning step<br>\n",
    "`q_tau`: Soft update factor for the Q Network, applied once every learning step<br>\n",
    "\n",
    "#### Training Parameters\n",
    "`max_eps`: Maximum number of episodes for which to train<br>\n",
    "`avg_wnd_len`: Length (in episodes) of running average buffer for reported performance<br>\n",
    "`rprt_int`: Number of episodes between prints of performance<br>\n",
    "`slv_thresh`: Minimum average score constituting solution of environment, the achievement of which will end the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf8986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Hyperparameters():\n",
    "    def __init__(self,\n",
    "                 state_dim=33,\n",
    "                 act_dim=4,\n",
    "                 num_agnt=20,\n",
    "                 pol_hid_num=1,\n",
    "                 pol_hid_size=300,\n",
    "                 noise_init=0.0,\n",
    "                 noise_decay=0.98,\n",
    "                 noise_theta=0.01,\n",
    "                 q_hid_num=1,\n",
    "                 q_hid_size=300,\n",
    "                 gamma=0.99,\n",
    "                 n_step_order=1,\n",
    "                 buf_life=1e6,\n",
    "                 buf_min_size=10000,\n",
    "                 alpha=0.7,\n",
    "                 beta=1.0,\n",
    "                 pol_lr=2e-5,\n",
    "                 q_lr=2e-4,\n",
    "                 lr_int=10,\n",
    "                 lr_stps=20,\n",
    "                 batch_size=32,\n",
    "                 p_tau=1e-3,\n",
    "                 q_tau=1e-3,\n",
    "                 max_eps=2000,\n",
    "                 avg_wnd_len=100,\n",
    "                 rprt_int=2,\n",
    "                 slv_thresh=30):\n",
    "        self.state_dim, self.act_dim, self.num_agnt = state_dim, act_dim, num_agnt\n",
    "        self.pol_hid_num, self.pol_hid_size, self.noise_init = pol_hid_num, pol_hid_size, noise_init\n",
    "        self.q_hid_num, self.q_hid_size, self.gamma = q_hid_num, q_hid_size, gamma\n",
    "        self.n_step_order, self.buf_life, self.buf_min_size = n_step_order, buf_life, buf_min_size\n",
    "        self.alpha, self.beta, self.pol_lr = alpha, beta, pol_lr\n",
    "        self.q_lr, self.lr_int, self.lr_stps = q_lr, lr_int, lr_stps\n",
    "        self.batch_size, self.p_tau, self.q_tau = batch_size, p_tau, q_tau\n",
    "        self.max_eps, self.avg_wnd_len, self.rprt_int = max_eps, avg_wnd_len, rprt_int\n",
    "        self.slv_thresh, self.noise_decay, self.noise_theta = slv_thresh, noise_decay, noise_theta\n",
    "\n",
    "def_hyp = DDPG_Hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7faf4",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a675fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging function\n",
    "def tensor_check(input,desc,exp_size):\n",
    "    if torch.any(torch.isnan(input)):\n",
    "        print(f'NaNs in {desc}:')\n",
    "        print(input)\n",
    "    if torch.any(torch.isinf(input)):\n",
    "        print(f'Inf in {desc}:')\n",
    "        print(input)\n",
    "    if not (input.size() == torch.Size(exp_size)):\n",
    "        print(f'{desc} has size {tuple(input.size())}, not {exp_size} expected')\n",
    "\n",
    "# Copied from the Lunar Lander dqn_agent.py file of the Udacity repo for course\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "# Object that represents an experience in the experience buffer or a non-leaf node of the sum tree\n",
    "# Experience tuples are stored in the self.data attribute\n",
    "# Based on code in Reference [4]\n",
    "class SumTreeNode():\n",
    "    \n",
    "    def __init__(self,data=None,p_i=0):\n",
    "        self.data = data\n",
    "        self.p_i = p_i\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "    \n",
    "    def update_p(self, delta_p):\n",
    "        self.p_i += delta_p\n",
    "        if self.parent is not None:\n",
    "            self.parent.update_p(delta_p)\n",
    "    \n",
    "    def attach_child(self,child):\n",
    "        if self.data is None:    # Not a leaf node\n",
    "            if self.left_child is None:    # No children, become leaf with cloned data\n",
    "                self.data = child.data\n",
    "                self.update_p(child.p_i - self.p_i)\n",
    "            else:    # Non-leaf node, attach to lower p_i side\n",
    "                if self.left_child.p_i < self.right_child.p_i:\n",
    "                    delegate_node = self.left_child\n",
    "                else:\n",
    "                    delegate_node = self.right_child\n",
    "                delegate_node.attach_child(child)\n",
    "        else:    # self is a leaf-node.  Clone self.data into new child, become non-leaf\n",
    "            self.left_child = SumTreeNode(self.data,self.p_i)\n",
    "            self.data = None\n",
    "            self.right_child = child\n",
    "            self.left_child.parent, self.right_child.parent = self, self     \n",
    "            self.update_p((self.left_child.p_i + self.right_child.p_i)- self.p_i)\n",
    "    \n",
    "    def weighted_retrieve(self,p_samp):\n",
    "        if self.data is not None: # must be a leaf-node\n",
    "            return self\n",
    "        else:\n",
    "            if self.left_child.p_i >= p_samp:\n",
    "                return self.left_child.weighted_retrieve(p_samp)\n",
    "            else:\n",
    "                return self.right_child.weighted_retrieve(p_samp - self.left_child.p_i)\n",
    "        \n",
    "# Experience aggregate\n",
    "Experience = namedtuple('Experience',['state','action','reward','last_state'])\n",
    "        \n",
    "class PrioritizedReplayBuffer():\n",
    "    \n",
    "    def __init__(self,hyp):\n",
    "        self.buf_life = hyp.buf_life\n",
    "        self.alpha = hyp.alpha\n",
    "        self.store = SumTreeNode()\n",
    "        self.sample_count = 0\n",
    "        self.exp_count = 0\n",
    "        self.beta = hyp.beta\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.exp_count\n",
    "    \n",
    "    def add_experience(self, experience, loss):\n",
    "        new_p_i = pow(loss, self.alpha)\n",
    "        self.store.attach_child(SumTreeNode(experience, new_p_i))\n",
    "        self.exp_count += 1\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        sample_keys = (np.random.rand(batch_size)*self.store.p_i).tolist()\n",
    "        samples = ([self.store.weighted_retrieve(p_samp) for p_samp in sample_keys],\n",
    "                   (self.exp_count,self.store.p_i))\n",
    "        self.sample_count += 1\n",
    "        if (self.sample_count >= self.buf_life):\n",
    "            self.sample_count = 0\n",
    "            self.store = SumTreeNode()\n",
    "            self.exp_count = 0\n",
    "            print ('\\nFlushed replay buffer!\\n')\n",
    "        return samples\n",
    "    \n",
    "# Circular buffer for generation of n_step rewards\n",
    "class MultistepBuffer():\n",
    "    def __init__(self,hyp):\n",
    "        self.n_step_order = hyp.n_step_order\n",
    "        self.store = deque(maxlen = hyp.n_step_order + 1)\n",
    "        self.gamma = hyp.gamma\n",
    "    \n",
    "    def add_experience(self,exp):\n",
    "        self.store.append(exp)\n",
    "    \n",
    "    def ready(self):\n",
    "        return len(self.store) == (self.n_step_order + 1)\n",
    "    \n",
    "    def get_n_step_experience(self):\n",
    "        out_state = self.store[0].state\n",
    "        out_action = self.store[0].action\n",
    "        out_reward = \\\n",
    "            sum([((self.store[i].reward) * pow(self.gamma,i)) for i in range(self.n_step_order)])\n",
    "        out_final_state = self.store[-1].state\n",
    "        return SumTreeNode(Experience(out_state, out_action, out_reward, out_final_state),p_i=1)\n",
    "    \n",
    "# Logger for running average\n",
    "class PerformanceLogger():\n",
    "    def __init__(self,avg_wnd_len=100,starting_scores=None):\n",
    "        self.avg_wnd_len = avg_wnd_len\n",
    "        self.scores = starting_scores if starting_scores is not None else []\n",
    "        self.internal_run_avg = 0\n",
    "        \n",
    "    def add_score(self,score):\n",
    "        self.scores.append(score)\n",
    "        self.internal_run_avg += score / self.avg_wnd_len\n",
    "        # Remove tail of running average\n",
    "        if len(self.scores) > self.avg_wnd_len:\n",
    "            self.internal_run_avg -= self.scores[-(self.avg_wnd_len + 1)] / self.avg_wnd_len\n",
    "    \n",
    "    def has_full_window(self):\n",
    "        return len(self.scores) >= self.avg_wnd_len\n",
    "    \n",
    "    def run_avg(self):\n",
    "        return self.internal_run_avg \\\n",
    "                * (1 if self.has_full_window() else (self.avg_wnd_len/len(self.scores)))\n",
    "    \n",
    "# Ohrstein-Uhlenbeck Process Noise Generator\n",
    "class OhrsteinUhlenbeckGen():\n",
    "    def __init__(self,out_dim=1,theta=1.0,sigma=1.0):\n",
    "        self.state = torch.zeros(out_dim,dtype=torch.float32)\n",
    "        self.out_dim = out_dim\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "    def sample(self):\n",
    "        step_noise = self.sigma * \\\n",
    "                         torch.clamp(torch.randn(self.out_dim,dtype=torch.float32),-5.0,5.0)\n",
    "        self.state = ((1.0 - self.theta) * self.state) + step_noise\n",
    "        return step_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62594cf0",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff21130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic MLP\n",
    "class DDPG_Subnet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_dim, \n",
    "                 out_dim, \n",
    "                 hid_size, \n",
    "                 num_hid,\n",
    "                 squish_output=False\n",
    "                 ):\n",
    "        super(DDPG_Subnet,self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_dim, hid_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(num_hid-1):\n",
    "            layers.append(nn.Linear(hid_size, hid_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hid_size, out_dim))\n",
    "        \n",
    "        self.reg_layers = nn.Sequential(*layers)\n",
    "        self.squish_output = squish_output\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.float()\n",
    "        x = self.reg_layers(x)\n",
    "        if self.squish_output:\n",
    "            x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "# Generators\n",
    "def new_pol_net(hyp):\n",
    "    return DDPG_Subnet(hyp.state_dim,\n",
    "                       hyp.act_dim,\n",
    "                       hyp.pol_hid_size,\n",
    "                       hyp.pol_hid_num,\n",
    "                       True)\n",
    "                       \n",
    "def new_q_net(hyp):\n",
    "    return DDPG_Subnet(hyp.state_dim + hyp.act_dim,\n",
    "                       1,\n",
    "                       hyp.q_hid_size,\n",
    "                       hyp.q_hid_num,\n",
    "                       False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabdd11",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3caa249",
   "metadata": {},
   "source": [
    "#### `train_net` Parameters\n",
    "`q_net`: Q network to use for training run<br>\n",
    "`pol_net`: Policy network to use for training run<br>\n",
    "`env`: Environment to use for training run<br>\n",
    "`hyp`: `DDPG_Hyperparameters` object to use for training run<br>\n",
    "\n",
    "#### `train_net` Returns\n",
    "Reference to a `PerformanceLogger` with the score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd32713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to compute importance sampling weight corrections\n",
    "def comp_w_i(experiences, hyp, device):\n",
    "    \n",
    "    p_i_total = experiences[1][1]\n",
    "    buf_N = experiences[1][0]\n",
    "    # Normalized p_i see eqn (1) of reference [3]\n",
    "    p_i = torch.tensor([(e.p_i/p_i_total) for e in experiences[0]]).to(device)\n",
    "    p_i = torch.clamp(p_i,min=1e-9)\n",
    "    w_i = torch.pow(torch.reciprocal(torch.mul(buf_N,p_i)),hyp.beta).unsqueeze(dim=1)\n",
    "    w_i = torch.clamp(w_i,1e-8,1.0)\n",
    "    \n",
    "    return w_i\n",
    "\n",
    "def Q_Loss(q_net, q_trg, pol_trg, experiences, hyp, device):\n",
    "    \n",
    "    init_states = torch.tensor(np.array([e.data.state for e in experiences[0]])).to(device)\n",
    "    actions = torch.vstack([e.data.action for e in experiences[0]]).to(device)\n",
    "    rewards = torch.tensor([e.data.reward for e in experiences[0]]).unsqueeze(dim=1).to(device)\n",
    "    final_states = torch.tensor(np.array([e.data.last_state for e in experiences[0]])).to(device)\n",
    "\n",
    "    q_net_input = torch.cat((init_states,actions),dim=1).float()\n",
    "    #tensor_check(q_net_input,'Q Loss q_net_input',(len(experiences[0]),hyp.state_dim + hyp.act_dim))\n",
    "    q_net_output = q_net(q_net_input)\n",
    "    \n",
    "    q_trg_input = torch.cat((final_states,pol_trg(final_states)),dim=1).float()\n",
    "    #tensor_check(q_trg_input,'q_trg_input',(len(experiences[0]),hyp.state_dim + hyp.act_dim))\n",
    "    q_trg_output = q_trg(q_trg_input)\n",
    "    \n",
    "    disc = pow(hyp.gamma,hyp.n_step_order)\n",
    "    #tensor_check(rewards,'rewards',(len(experiences[0]),1))\n",
    "    q_loss = (rewards + (disc * q_trg_output)) - q_net_output\n",
    "    q_loss = torch.pow(q_loss,2)\n",
    "    #tensor_check(q_loss,'q_loss',(len(experiences[0]),1))\n",
    "    \n",
    "    # Importance sampling weights\n",
    "    w_i = comp_w_i(experiences, hyp, device)\n",
    "    #tensor_check(w_i,'w_i',(len(experiences[0]),1))\n",
    "    \n",
    "    return (torch.mul(q_loss,w_i), w_i)\n",
    "    \n",
    "def Pol_Loss(q_net, pol_net, experiences, hyp, device):\n",
    "    \n",
    "    init_states = torch.tensor(np.array([e.data.state for e in experiences[0]])).to(device)\n",
    "    w_i = comp_w_i(experiences, hyp, device)\n",
    "    q_net_input = torch.cat((init_states,pol_net(init_states)),dim=1).float()\n",
    "    #tensor_check(q_net_input,'Pol Loss q_net_input',(len(experiences[0]),hyp.state_dim+hyp.act_dim))\n",
    "    q_net_output = q_net(q_net_input)\n",
    "    out_loss = torch.mul(-1.0,torch.mean(torch.mul(q_net_output,w_i)))\n",
    "    #tensor_check(out_loss,'out_loss',())\n",
    "    return out_loss\n",
    "    \n",
    "def train_net(pol_net, q_net, env, hyp):\n",
    "    \n",
    "    # Use gpu if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Move primary models to device\n",
    "    pol_net = pol_net.to(device)\n",
    "    q_net = q_net.to(device)\n",
    "    \n",
    "    # Generate target policy and q networks\n",
    "    pol_trg = new_pol_net(hyp).to(device)\n",
    "    q_trg = new_q_net(hyp).to(device)\n",
    "    \n",
    "    # Initialize target networks, copy policy, zeroed q network\n",
    "    soft_update(pol_net, pol_trg, 1.0)\n",
    "    for param in q_trg.parameters():\n",
    "        param.data.fill_(0)\n",
    "        \n",
    "    # Setup optimizers\n",
    "    pol_optim = Adam(pol_net.parameters(), lr=hyp.pol_lr)\n",
    "    q_optim = Adam(q_net.parameters(), lr=hyp.q_lr)\n",
    "    \n",
    "    # Init counters\n",
    "    lrn_cntr = 0\n",
    "    rpt_cntr = 0\n",
    "    \n",
    "    # Unity ML-Agents Setup\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    replay_buffer=PrioritizedReplayBuffer(hyp)\n",
    "    perf_log = PerformanceLogger(avg_wnd_len = hyp.avg_wnd_len)\n",
    "    \n",
    "    # Put all networks in train mode as there are batch norm layers\n",
    "    pol_net.train()\n",
    "    pol_trg.train()\n",
    "    q_net.train()\n",
    "    q_trg.train()\n",
    "    \n",
    "    # Noise generator\n",
    "    noise_gen = OhrsteinUhlenbeckGen(hyp.act_dim*hyp.num_agnt,hyp.noise_theta,1.0)\n",
    "    noise_factor = hyp.noise_init\n",
    "    noise_decay = hyp.noise_decay\n",
    "    \n",
    "    for episode in range(hyp.max_eps):\n",
    "        \n",
    "        n_step_bufs = [MultistepBuffer(hyp) for i in range(hyp.num_agnt)]\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = [0]*hyp.num_agnt\n",
    "        dones = [False]*hyp.num_agnt\n",
    "        \n",
    "        while dones.count(True) < hyp.num_agnt:\n",
    "            \n",
    "            pol_net.eval()\n",
    "            actions = pol_net(torch.tensor(states,device=device)).cpu().detach()\n",
    "            act_noise = noise_factor*noise_gen.sample().reshape((hyp.num_agnt, hyp.act_dim))\n",
    "            actions = torch.clamp(torch.add(actions,act_noise),-1.0,1.0)\n",
    "            pol_net.train()\n",
    "            env_info = env.step(actions.numpy())[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            env_dones = env_info.local_done\n",
    "            n_step_exps = []\n",
    "            for i in range(hyp.num_agnt):\n",
    "                \n",
    "                if dones[i] == False:\n",
    "                    \n",
    "                    scores[i] += rewards[i]\n",
    "                    n_step_bufs[i].add_experience(Experience(states[i],actions[i],rewards[i],None))\n",
    "                    \n",
    "                    if n_step_bufs[i].ready():\n",
    "                        \n",
    "                        n_step_exps.append(n_step_bufs[i].get_n_step_experience())\n",
    "                    \n",
    "                    dones[i] = env_dones[i]\n",
    "            \n",
    "            # Compute losses to determine sampling priorities, add new states to replay buffer\n",
    "            if len(n_step_exps) > 0:\n",
    "                q_net.eval()\n",
    "                q_trg.eval()\n",
    "                pol_trg.eval()\n",
    "                loss, w_i = Q_Loss(q_net, q_trg, pol_trg, \n",
    "                                   (n_step_exps,(len(n_step_exps),len(n_step_exps))), \n",
    "                                   hyp, device)\n",
    "                pol_trg.train()\n",
    "                q_trg.train()\n",
    "                q_net.train()\n",
    "                p_i = torch.div(loss,w_i).squeeze(dim=1)\n",
    "                for j in range(len(n_step_exps)):\n",
    "                    replay_buffer.add_experience(n_step_exps[j].data,p_i[j].item())\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            lrn_cntr += 1\n",
    "            if (lrn_cntr % hyp.lr_int) == 0:\n",
    "                \n",
    "                lrn_cntr = 0\n",
    "                if len(replay_buffer) > max(hyp.buf_min_size, hyp.batch_size):\n",
    "                    \n",
    "                    q_perf_log = PerformanceLogger()\n",
    "                    pol_perf_log = PerformanceLogger()\n",
    "                    for l_step in range(hyp.lr_stps):\n",
    "                        \n",
    "                        # Check that a sample op did not trigger reset\n",
    "                        if len(replay_buffer) < max(hyp.buf_min_size, hyp.batch_size):\n",
    "                            break\n",
    "                            \n",
    "                        samp_exp = replay_buffer.sample(hyp.batch_size)\n",
    "                        \n",
    "                        # Q Network Update\n",
    "                        q_optim.zero_grad()\n",
    "                        loss, w_i = Q_Loss(q_net, q_trg, pol_trg, samp_exp, hyp, device)\n",
    "                        mean_loss = torch.mean(loss)\n",
    "                        mean_loss.backward()\n",
    "                        q_optim.step()\n",
    "                        q_perf_log.add_score(mean_loss.item())\n",
    "                        new_p_i = torch.div(loss,w_i).pow(hyp.alpha).squeeze(dim=1)\n",
    "                        \n",
    "                        # Policy Network Update\n",
    "                        q_optim.zero_grad()\n",
    "                        pol_optim.zero_grad()\n",
    "                        loss = Pol_Loss(q_net, pol_net, samp_exp, hyp, device)\n",
    "                        loss.backward()\n",
    "                        pol_optim.step()\n",
    "                        pol_perf_log.add_score(loss.item())\n",
    "                        \n",
    "                        # Update priorities in replay buffer according to losses\n",
    "                        for exp_num in range(hyp.batch_size):\n",
    "                            samp_exp[0][exp_num].update_p( \\\n",
    "                                (new_p_i[exp_num].item() - samp_exp[0][exp_num].p_i))\n",
    "                            \n",
    "                        # Update target networks\n",
    "                        soft_update(q_net, q_trg, hyp.q_tau)\n",
    "                        soft_update(pol_net, pol_trg, hyp.p_tau)\n",
    "                        \n",
    "                        # Output status of optimization periodically\n",
    "                        if ((l_step + 1) % (hyp.lr_stps / 10) == 0):\n",
    "                            q_avg = q_perf_log.run_avg()\n",
    "                            p_avg = pol_perf_log.run_avg()\n",
    "                            print(f'Completed {l_step + 1} of {hyp.lr_stps} learning steps. ' +\n",
    "                                  f'Q Loss = {q_avg:.5f}, Policy Value = {-p_avg:.5f}', end='\\r')\n",
    "        \n",
    "        perf_log.add_score(sum(scores)/len(scores))\n",
    "        noise_factor = noise_factor * noise_decay\n",
    "        \n",
    "        if perf_log.has_full_window() and (perf_log.run_avg() >= hyp.slv_thresh):\n",
    "            print(f'Solved with average score of {perf_log.run_avg()} in {episode+1} episodes')\n",
    "            break\n",
    "        \n",
    "        rpt_cntr += 1\n",
    "        if (rpt_cntr % hyp.rprt_int) == 0:\n",
    "            rpt_cntr = 0\n",
    "            print(f'\\nCompleted {episode + 1} episodes. Average score = {perf_log.run_avg():.3f}')\n",
    "        \n",
    "        if episode == (hyp.max_eps - 1):\n",
    "            print(f'Failed to solve within the maximum of {max_eps} episodes')\n",
    "            break\n",
    "    \n",
    "    return perf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83f325",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e339958c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 20 of 20 learning steps. Q Loss = 0.00184, Policy Value = 0.03483\n",
      "Completed 2 episodes. Average score = 0.320\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00207, Policy Value = 0.04807\n",
      "Completed 4 episodes. Average score = 0.572\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00075, Policy Value = 0.06266\n",
      "Completed 6 episodes. Average score = 0.588\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00050, Policy Value = 0.06868\n",
      "Completed 8 episodes. Average score = 0.677\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00034, Policy Value = 0.07858\n",
      "Completed 10 episodes. Average score = 0.721\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00021, Policy Value = 0.08535\n",
      "Completed 12 episodes. Average score = 0.796\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00025, Policy Value = 0.09025\n",
      "Completed 14 episodes. Average score = 0.842\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00012, Policy Value = 0.09132\n",
      "Completed 16 episodes. Average score = 0.849\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00010, Policy Value = 0.09183\n",
      "Completed 18 episodes. Average score = 0.870\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00012, Policy Value = 0.09787\n",
      "Completed 20 episodes. Average score = 0.897\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00010, Policy Value = 0.09596\n",
      "Completed 22 episodes. Average score = 0.960\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00008, Policy Value = 0.10266\n",
      "Completed 24 episodes. Average score = 0.997\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00010, Policy Value = 0.09929\n",
      "Completed 26 episodes. Average score = 1.065\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00011, Policy Value = 0.10151\n",
      "Completed 28 episodes. Average score = 1.143\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00010, Policy Value = 0.09960\n",
      "Completed 30 episodes. Average score = 1.219\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00013, Policy Value = 0.10618\n",
      "Completed 32 episodes. Average score = 1.290\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00013, Policy Value = 0.10949\n",
      "Completed 34 episodes. Average score = 1.345\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00014, Policy Value = 0.11417\n",
      "Completed 36 episodes. Average score = 1.431\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00014, Policy Value = 0.11520\n",
      "Completed 38 episodes. Average score = 1.498\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00022, Policy Value = 0.11843\n",
      "Completed 40 episodes. Average score = 1.582\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00017, Policy Value = 0.12191\n",
      "Completed 42 episodes. Average score = 1.675\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00018, Policy Value = 0.12073\n",
      "Completed 44 episodes. Average score = 1.775\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00017, Policy Value = 0.13183\n",
      "Completed 46 episodes. Average score = 1.881\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00019, Policy Value = 0.14157\n",
      "Completed 48 episodes. Average score = 2.009\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00021, Policy Value = 0.14465\n",
      "Completed 50 episodes. Average score = 2.147\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00026, Policy Value = 0.14900\n",
      "Completed 52 episodes. Average score = 2.247\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00022, Policy Value = 0.16171\n",
      "Completed 54 episodes. Average score = 2.361\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00025, Policy Value = 0.17660\n",
      "Completed 56 episodes. Average score = 2.527\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00030, Policy Value = 0.17251\n",
      "Completed 58 episodes. Average score = 2.688\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00031, Policy Value = 0.18566\n",
      "Completed 60 episodes. Average score = 2.839\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00041, Policy Value = 0.19444\n",
      "Completed 62 episodes. Average score = 3.013\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00031, Policy Value = 0.21199\n",
      "Completed 64 episodes. Average score = 3.178\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00032, Policy Value = 0.21869\n",
      "Completed 66 episodes. Average score = 3.337\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00039, Policy Value = 0.21891\n",
      "Completed 68 episodes. Average score = 3.464\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00049, Policy Value = 0.22994\n",
      "Completed 70 episodes. Average score = 3.632\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00042, Policy Value = 0.23410\n",
      "Completed 72 episodes. Average score = 3.853\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00044, Policy Value = 0.23669\n",
      "Completed 74 episodes. Average score = 4.020\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00049, Policy Value = 0.26514\n",
      "Completed 76 episodes. Average score = 4.207\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00044, Policy Value = 0.26520\n",
      "Completed 78 episodes. Average score = 4.392\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00057, Policy Value = 0.28289\n",
      "Completed 80 episodes. Average score = 4.567\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00056, Policy Value = 0.28158\n",
      "Completed 82 episodes. Average score = 4.689\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00055, Policy Value = 0.30166\n",
      "Completed 84 episodes. Average score = 4.814\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00058, Policy Value = 0.30614\n",
      "Completed 86 episodes. Average score = 4.971\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00053, Policy Value = 0.33701\n",
      "Completed 88 episodes. Average score = 5.110\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00064, Policy Value = 0.31940\n",
      "Completed 90 episodes. Average score = 5.360\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00064, Policy Value = 0.35475\n",
      "Completed 92 episodes. Average score = 5.555\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00067, Policy Value = 0.38808\n",
      "Completed 94 episodes. Average score = 5.748\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00059, Policy Value = 0.37355\n",
      "Completed 96 episodes. Average score = 5.934\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00067, Policy Value = 0.38967\n",
      "Completed 98 episodes. Average score = 6.169\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00098, Policy Value = 0.41307\n",
      "Completed 100 episodes. Average score = 6.411\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00086, Policy Value = 0.41255\n",
      "Completed 102 episodes. Average score = 6.740\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00080, Policy Value = 0.41795\n",
      "Completed 104 episodes. Average score = 7.122\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00089, Policy Value = 0.45484\n",
      "Completed 106 episodes. Average score = 7.504\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00079, Policy Value = 0.46556\n",
      "Completed 108 episodes. Average score = 7.870\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00113, Policy Value = 0.47016\n",
      "Completed 110 episodes. Average score = 8.215\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00082, Policy Value = 0.50338\n",
      "Completed 112 episodes. Average score = 8.623\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00085, Policy Value = 0.52930\n",
      "Completed 114 episodes. Average score = 9.034\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00093, Policy Value = 0.51918\n",
      "Completed 116 episodes. Average score = 9.498\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00107, Policy Value = 0.52778\n",
      "Completed 118 episodes. Average score = 9.974\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00094, Policy Value = 0.55629\n",
      "Completed 120 episodes. Average score = 10.481\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00105, Policy Value = 0.56949\n",
      "Completed 122 episodes. Average score = 10.878\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00106, Policy Value = 0.59285\n",
      "Completed 124 episodes. Average score = 11.358\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00235, Policy Value = 0.63508\n",
      "Completed 126 episodes. Average score = 11.760\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00100, Policy Value = 0.66182\n",
      "Completed 128 episodes. Average score = 12.275\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00119, Policy Value = 0.67693\n",
      "Completed 130 episodes. Average score = 12.780\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00119, Policy Value = 0.67093\n",
      "Completed 132 episodes. Average score = 13.248\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00100, Policy Value = 0.71269\n",
      "Completed 134 episodes. Average score = 13.662\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00136, Policy Value = 0.75265\n",
      "Completed 136 episodes. Average score = 14.066\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00111, Policy Value = 0.72629\n",
      "Completed 138 episodes. Average score = 14.520\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00119, Policy Value = 0.75894\n",
      "Completed 140 episodes. Average score = 14.982\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00127, Policy Value = 0.80209\n",
      "Completed 142 episodes. Average score = 15.493\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00127, Policy Value = 0.80147\n",
      "Completed 144 episodes. Average score = 15.942\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00161, Policy Value = 0.77562\n",
      "Completed 146 episodes. Average score = 16.385\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00163, Policy Value = 0.85456\n",
      "Completed 148 episodes. Average score = 16.838\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00128, Policy Value = 0.89719\n",
      "Completed 150 episodes. Average score = 17.274\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00132, Policy Value = 0.92382\n",
      "Completed 152 episodes. Average score = 17.711\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00130, Policy Value = 0.86972\n",
      "Completed 154 episodes. Average score = 18.159\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00122, Policy Value = 0.90697\n",
      "Completed 156 episodes. Average score = 18.582\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00152, Policy Value = 0.95787\n",
      "Completed 158 episodes. Average score = 19.034\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00146, Policy Value = 0.91535\n",
      "Completed 160 episodes. Average score = 19.479\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00139, Policy Value = 1.05544\n",
      "Completed 162 episodes. Average score = 19.906\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00136, Policy Value = 1.04696\n",
      "Completed 164 episodes. Average score = 20.340\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00164, Policy Value = 1.00862\n",
      "Completed 166 episodes. Average score = 20.711\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00151, Policy Value = 1.02643\n",
      "Completed 168 episodes. Average score = 21.105\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00132, Policy Value = 1.06460\n",
      "Completed 170 episodes. Average score = 21.456\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00170, Policy Value = 1.08480\n",
      "Completed 172 episodes. Average score = 21.811\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00154, Policy Value = 1.10193\n",
      "Completed 174 episodes. Average score = 22.218\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00162, Policy Value = 1.09498\n",
      "Completed 176 episodes. Average score = 22.571\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00174, Policy Value = 1.13722\n",
      "Completed 178 episodes. Average score = 22.970\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00127, Policy Value = 1.14565\n",
      "Completed 180 episodes. Average score = 23.348\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00139, Policy Value = 1.12952\n",
      "Completed 182 episodes. Average score = 23.767\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00145, Policy Value = 1.22609\n",
      "Completed 184 episodes. Average score = 24.180\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00137, Policy Value = 1.13829\n",
      "Completed 186 episodes. Average score = 24.529\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00127, Policy Value = 1.26399\n",
      "Completed 188 episodes. Average score = 24.936\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00129, Policy Value = 1.26694\n",
      "Completed 190 episodes. Average score = 25.293\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00130, Policy Value = 1.29874\n",
      "Completed 192 episodes. Average score = 25.660\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00159, Policy Value = 1.26228\n",
      "Completed 194 episodes. Average score = 26.021\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00106, Policy Value = 1.22081\n",
      "Completed 196 episodes. Average score = 26.387\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00153, Policy Value = 1.26986\n",
      "Completed 198 episodes. Average score = 26.655\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00122, Policy Value = 1.37446\n",
      "Completed 200 episodes. Average score = 26.963\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00135, Policy Value = 1.45545\n",
      "Completed 202 episodes. Average score = 27.331\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00107, Policy Value = 1.38376\n",
      "Completed 204 episodes. Average score = 27.618\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00146, Policy Value = 1.42252\n",
      "Completed 206 episodes. Average score = 27.897\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00121, Policy Value = 1.37986\n",
      "Completed 208 episodes. Average score = 28.240\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00147, Policy Value = 1.45536\n",
      "Completed 210 episodes. Average score = 28.588\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00133, Policy Value = 1.42657\n",
      "Completed 212 episodes. Average score = 28.911\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00101, Policy Value = 1.45117\n",
      "Completed 214 episodes. Average score = 29.221\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00123, Policy Value = 1.52463\n",
      "Completed 216 episodes. Average score = 29.482\n",
      "Completed 20 of 20 learning steps. Q Loss = 0.00115, Policy Value = 1.41320\n",
      "Completed 218 episodes. Average score = 29.767\n",
      "Solved with average score of 30.013614329143422 in 220 episodesue = 1.57725\n"
     ]
    }
   ],
   "source": [
    "pol_net = new_pol_net(def_hyp)\n",
    "q_net = new_q_net(def_hyp)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(pol_net, q_net, env, def_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd0dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final models\n",
    "torch.save(pol_net,\"pol_net_final.pt\")\n",
    "torch.save(q_net,\"q_net_final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feae780",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d84f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rslt.scores)\n",
    "plt.title('Training Results')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcc45c",
   "metadata": {},
   "source": [
    "### Demo\n",
    "he cell below can be run to demonstrate the behavior of the trained agent. It is necessary to first run all cells except the training cell. One can also review this [video](https://youtu.be/DvaDpi1xjLg) of the trained agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54045e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "demo_eps = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "demo_pol_net = torch.load('submission_pol_net.pt')\n",
    "demo_pol_net = demo_pol_net.to(device)\n",
    "demo_pol_net.eval()\n",
    "\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "for episode in range(demo_eps):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    dones = [False]*def_hyp.num_agnt\n",
    "    \n",
    "    while dones.count(True) < def_hyp.num_agnt:\n",
    "            \n",
    "            actions = demo_pol_net(torch.tensor(states,device=device)).cpu().detach()        \n",
    "            env_info = env.step(actions.numpy())[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            env_dones = env_info.local_done\n",
    "            \n",
    "            for i in range(def_hyp.num_agnt):\n",
    "                \n",
    "                if dones[i] == False:\n",
    "                    \n",
    "                    dones[i] = env_dones[i]\n",
    "            \n",
    "            states = next_states\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed94fb",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Added noise was not critical to solve this environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656ce7d",
   "metadata": {},
   "source": [
    "### Ideas for Future Work\n",
    "\n",
    "+ Distributional version of DDPG may be faster\n",
    "+ Train more than one policy network at a time, apply genetic algorithm techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adaa731",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Lillicrap et. al., Continuous control with deep reinforcement learning, [arXiv:1509.02971](https://arxiv.org/abs/1509.02971) <br>\n",
    "[2] Schaul et. al., Prioritized Experience Replay, [arXiv:1511.05952](https://arxiv.org/abs/1511.05952)<br>\n",
    "[3] Fortunato et. al., Noisy Networks for Exploration, [arXiv:1706.10295](https://arxiv.org/abs/1706.10295)<br>\n",
    "[4] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
